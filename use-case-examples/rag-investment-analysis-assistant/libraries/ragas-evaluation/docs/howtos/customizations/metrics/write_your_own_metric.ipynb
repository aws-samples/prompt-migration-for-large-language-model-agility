{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Ragas has [a number of built-in metrics](./../../../concepts/metrics/available_metrics/index.md), you may find yourself needing to create a custom metric for your use case. This guide will help you do just that. \n",
    "\n",
    "For the sake of this tutorial, let's assume we want to build a custom metric that measures the hallucinations in a LLM application. While we do have a built-in metric called [Faithfulness][ragas.metrics.Faithfulness] which is similar but not exactly the same. `Faithfulness` measures the factual consistency of the generated answer against the given context while `Hallucinations` measures the presence of hallucinations in the generated answer.\n",
    "\n",
    "before we start, lets load the dataset and define the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a6ed73cc60476980eecaf861810caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset\n",
    "from datasets import load_dataset\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v3\")\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(amnesty_qa[\"eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=20)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--8<--\n",
    "choose_evaluator_llm.md\n",
    "--8<--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import llm_factory\n",
    "\n",
    "evaluator_llm = llm_factory(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect Critic - Simple Criteria Scoring\n",
    "\n",
    "[Aspect Critic](./../../../concepts/metrics/available_metrics/aspect_critic.md) that outputs a binary score for `definition` you provide. A simple pass/fail metric can be bring clarity and focus to what you are trying to measure and is a better alocation of effort than building a more complex metric from scratch, especially when starting out. \n",
    "\n",
    "Check out these resources to learn more about the effectiveness of having a simple pass/fail metric:\n",
    "\n",
    "- [Hamel's Blog on Creating LLM-as-a-Judge that drives Business Result](https://hamel.dev/blog/posts/llm-judge/#step-3-direct-the-domain-expert-to-make-passfail-judgments-with-critiques)\n",
    "- [Eugene's Blog on AlignEval](https://eugeneyan.com/writing/aligneval/#labeling-mode-look-at-the-data)\n",
    "\n",
    "Now let's create a simple pass/fail metric to measure the hallucinations in the dataset with Ragas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "# you can init the metric with the evaluator llm\n",
    "hallucinations_binary = AspectCritic(\n",
    "    name=\"hallucinations_binary\",\n",
    "    definition=\"Did the model hallucinate or add any information that was not present in the retrieved context?\",\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "await hallucinations_binary.single_turn_ascore(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Specific Metrics or Rubric based Metrics\n",
    "\n",
    "Here we will build a rubric based metric that evaluates the data on a scale of 1 to 5 based on the rubric we provide. You can read more about the rubric based metrics [here](./../../../concepts/metrics/available_metrics/rubrics_based.md)\n",
    "\n",
    "For our example of building a hallucination metric, we will use the following rubric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = {\n",
    "    \"score1_description\": \"There is no hallucination in the response. All the information in the response is present in the retrieved context.\",\n",
    "    \"score2_description\": \"There are no factual statements that are not present in the retrieved context but the response is not fully accurate and lacks important details.\",\n",
    "    \"score3_description\": \"There are many factual statements that are not present in the retrieved context.\",\n",
    "    \"score4_description\": \"The response contains some factual errors and lacks important details.\",\n",
    "    \"score5_description\": \"The model adds new information and statements that contradict the retrieved context.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets init the metric with the rubric and evaluator llm and evaluate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import RubricsScore\n",
    "\n",
    "hallucinations_rubric = RubricsScore(\n",
    "    name=\"hallucinations_rubric\", llm=evaluator_llm, rubrics=rubric\n",
    ")\n",
    "\n",
    "await hallucinations_rubric.single_turn_ascore(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics\n",
    "\n",
    "If your use case is not covered by those two, you can build a custom metric by subclassing the base `Metric` class in Ragas but before that you have to ask yourself the following questions:\n",
    "\n",
    "1. Am I trying to build a single turn or multi turn metric? If yes, subclassing the `Metric` class along with either [SingleTurnMetric][ragas.metrics.base.SingleTurnMetric] or [MultiTurnMetric][ragas.metrics.base.MultiTurnMetric] depending on if you are evaluating single turn or multi turn interactions.\n",
    "\n",
    "2. Do I need to use LLMs to evaluate my metric? If yes, instead of subclassing the [Metric][ragas.metrics.base.Metric] class, subclassing the [MetricWithLLM][ragas.metrics.base.MetricWithLLM] class.\n",
    "\n",
    "3. Do I need to use embeddings to evaluate my metric? If yes, instead of subclassing the [Metric][ragas.metrics.base.Metric] class, subclassing the [MetricWithEmbeddings][ragas.metrics.base.MetricWithEmbeddings] class.\n",
    "\n",
    "4. Do I need to use both LLM and Embeddings to evaluate my metric? If yes, subclass both the [MetricWithLLM][ragas.metrics.base.MetricWithLLM] and [MetricWithEmbeddings][ragas.metrics.base.MetricWithEmbeddings] classes.\n",
    "\n",
    "\n",
    "For our example, we need to to use LLMs to evaluate our metric so we will subclass the [MetricWithLLM][ragas.metrics.base.MetricWithLLM] class and we are working for only single turn interactions for now so we will subclass the [SingleTurnMetric][ragas.metrics.base.SingleTurnMetric] class. \n",
    "\n",
    "As for the implementation, we will use the [Faithfulness][ragas.metrics.Faithfulness] metric to evaluate our metric to measure the hallucinations with the formula \n",
    "\n",
    "$$\n",
    "\\text{Hallucinations} = 1 - \\text{Faithfulness}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models import ChatBedrock\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "config = {\n",
    "    \"region_name\": \"us-west-2\",  # E.g. \"us-east-1\"\n",
    "    \"model_id\": 'anthropic.claude-3-sonnet-20240229-v1:0', \n",
    "    \"model_kwargs\": {\"temperature\": 0.4, \"max_tokens\": 5000}, # change max tokens if required\n",
    "}\n",
    "bedrock_model = ChatBedrock(\n",
    "    region_name=config[\"region_name\"],\n",
    "    endpoint_url=f\"https://bedrock-runtime.{config['region_name']}.amazonaws.com\",\n",
    "    model_id=config[\"model_id\"],\n",
    "    model_kwargs=config[\"model_kwargs\"],\n",
    ")\n",
    "\n",
    "# init the embeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    region_name=config[\"region_name\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to create a dataclass that subclasses `MetricWithLLM` and `SingleTurnMetric`\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# import the base classes\n",
    "from ragas.metrics.base import MetricWithLLM, SingleTurnMetric, MetricType\n",
    "from ragas.metrics import Faithfulness\n",
    "\n",
    "# import types\n",
    "import typing as t\n",
    "from ragas.callbacks import Callbacks\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HallucinationsMetric(MetricWithLLM, SingleTurnMetric):\n",
    "    # name of the metric\n",
    "    name: str = \"hallucinations_metric\"\n",
    "    # we need to define the required columns for the metric\n",
    "    _required_columns: t.Dict[MetricType, t.Set[str]] = field(\n",
    "        default_factory=lambda: {\n",
    "            MetricType.SINGLE_TURN: {\"user_input\", \"response\", \"retrieved_contexts\"}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # init the faithfulness metric\n",
    "        self.faithfulness_metric = Faithfulness(llm=self.llm)\n",
    "\n",
    "    async def _single_turn_ascore(\n",
    "        self, sample: SingleTurnSample, callbacks: Callbacks\n",
    "    ) -> float:\n",
    "        faithfulness_score = await self.faithfulness_metric.single_turn_ascore(\n",
    "            sample, callbacks\n",
    "        )\n",
    "        return 1 - faithfulness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'StringPromptValue' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m hallucinations_metric \u001b[38;5;241m=\u001b[39m HallucinationsMetric(llm\u001b[38;5;241m=\u001b[39mbedrock_model)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m hallucinations_metric\u001b[38;5;241m.\u001b[39msingle_turn_ascore(eval_dataset[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/metrics/base.py:502\u001b[0m, in \u001b[0;36mSingleTurnMetric.single_turn_ascore\u001b[0;34m(self, sample, callbacks, timeout)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    501\u001b[0m         rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/metrics/base.py:495\u001b[0m, in \u001b[0;36mSingleTurnMetric.single_turn_ascore\u001b[0;34m(self, sample, callbacks, timeout)\u001b[0m\n\u001b[1;32m    488\u001b[0m rm, group_cm \u001b[38;5;241m=\u001b[39m new_group(\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    490\u001b[0m     inputs\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    491\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    492\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: ChainType\u001b[38;5;241m.\u001b[39mMETRIC},\n\u001b[1;32m    493\u001b[0m )\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_turn_ascore(sample\u001b[38;5;241m=\u001b[39msample, callbacks\u001b[38;5;241m=\u001b[39mgroup_cm),\n\u001b[1;32m    497\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/asyncio/tasks.py:408\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    405\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    411\u001b[0m     fut \u001b[38;5;241m=\u001b[39m ensure_future(fut, loop\u001b[38;5;241m=\u001b[39mloop)\n",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m, in \u001b[0;36mHallucinationsMetric._single_turn_ascore\u001b[0;34m(self, sample, callbacks)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_single_turn_ascore\u001b[39m(\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m, sample: SingleTurnSample, callbacks: Callbacks\n\u001b[1;32m     31\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     faithfulness_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaithfulness_metric\u001b[38;5;241m.\u001b[39msingle_turn_ascore(\n\u001b[1;32m     33\u001b[0m         sample, callbacks\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m faithfulness_score\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/metrics/base.py:502\u001b[0m, in \u001b[0;36mSingleTurnMetric.single_turn_ascore\u001b[0;34m(self, sample, callbacks, timeout)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[1;32m    501\u001b[0m         rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/metrics/base.py:495\u001b[0m, in \u001b[0;36mSingleTurnMetric.single_turn_ascore\u001b[0;34m(self, sample, callbacks, timeout)\u001b[0m\n\u001b[1;32m    488\u001b[0m rm, group_cm \u001b[38;5;241m=\u001b[39m new_group(\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    490\u001b[0m     inputs\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m    491\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    492\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: ChainType\u001b[38;5;241m.\u001b[39mMETRIC},\n\u001b[1;32m    493\u001b[0m )\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 495\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_single_turn_ascore(sample\u001b[38;5;241m=\u001b[39msample, callbacks\u001b[38;5;241m=\u001b[39mgroup_cm),\n\u001b[1;32m    497\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m group_cm\u001b[38;5;241m.\u001b[39mended:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/asyncio/tasks.py:408\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    405\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    411\u001b[0m     fut \u001b[38;5;241m=\u001b[39m ensure_future(fut, loop\u001b[38;5;241m=\u001b[39mloop)\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/metrics/_faithfulness.py:238\u001b[0m, in \u001b[0;36mFaithfulness._single_turn_ascore\u001b[0;34m(self, sample, callbacks)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_single_turn_ascore\u001b[39m(\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m, sample: SingleTurnSample, callbacks: Callbacks\n\u001b[1;32m    236\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    237\u001b[0m     row \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ascore(row, callbacks)\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/metrics/_faithfulness.py:247\u001b[0m, in \u001b[0;36mFaithfulness._ascore\u001b[0;34m(self, row, callbacks)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03mreturns the NLI score for each (q, c, a) pair\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM is not set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 247\u001b[0m statements_simplified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_statements(row, callbacks)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m statements_simplified \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/metrics/_faithfulness.py:211\u001b[0m, in \u001b[0;36mFaithfulness._create_statements\u001b[0;34m(self, row, callbacks)\u001b[0m\n\u001b[1;32m    208\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_segmenter\u001b[38;5;241m.\u001b[39msegment(text)\n\u001b[1;32m    209\u001b[0m sentences_with_index \u001b[38;5;241m=\u001b[39m {i: sentence \u001b[38;5;28;01mfor\u001b[39;00m i, sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sentences)}\n\u001b[0;32m--> 211\u001b[0m statements_simplified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatement_prompt\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    212\u001b[0m     llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm,\n\u001b[1;32m    213\u001b[0m     data\u001b[38;5;241m=\u001b[39mFaithfulnessStatements(\n\u001b[1;32m    214\u001b[0m         question\u001b[38;5;241m=\u001b[39mquestion, answer\u001b[38;5;241m=\u001b[39mtext, sentences\u001b[38;5;241m=\u001b[39msentences_with_index\n\u001b[1;32m    215\u001b[0m     ),\n\u001b[1;32m    216\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m statements_simplified\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/prompt/pydantic_prompt.py:126\u001b[0m, in \u001b[0;36mPydanticPrompt.generate\u001b[0;34m(self, llm, data, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    123\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# this is just a special case of generate_multiple\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m output_single \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_multiple(\n\u001b[1;32m    127\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m    128\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    129\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    130\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    131\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    132\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    133\u001b[0m     retries_left\u001b[38;5;241m=\u001b[39mretries_left,\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_single[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/SageMaker/ragas/src/ragas/prompt/pydantic_prompt.py:187\u001b[0m, in \u001b[0;36mPydanticPrompt.generate_multiple\u001b[0;34m(self, llm, data, n, temperature, stop, callbacks, retries_left)\u001b[0m\n\u001b[1;32m    180\u001b[0m prompt_rm, prompt_cb \u001b[38;5;241m=\u001b[39m new_group(\n\u001b[1;32m    181\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    182\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: processed_data},\n\u001b[1;32m    183\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    184\u001b[0m     metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: ChainType\u001b[38;5;241m.\u001b[39mRAGAS_PROMPT},\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m prompt_value \u001b[38;5;241m=\u001b[39m PromptValue(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_string(processed_data))\n\u001b[0;32m--> 187\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_cb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m output_models \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    196\u001b[0m parser \u001b[38;5;241m=\u001b[39m RagasOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_model)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:627\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m inheritable_metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(metadata \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ls_params(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    609\u001b[0m }\n\u001b[1;32m    611\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    612\u001b[0m     callbacks,\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    620\u001b[0m run_managers \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chat_model_start(\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m    622\u001b[0m     messages,\n\u001b[1;32m    623\u001b[0m     invocation_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    624\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    625\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    626\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[0;32m--> 627\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    628\u001b[0m )\n\u001b[1;32m    629\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'StringPromptValue' has no len()"
     ]
    }
   ],
   "source": [
    "hallucinations_metric = HallucinationsMetric(llm=bedrock_model)\n",
    "\n",
    "await hallucinations_metric.single_turn_ascore(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the entire dataset with the metrics we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "results = evaluate(\n",
    "    eval_dataset,\n",
    "    metrics=[hallucinations_metric, hallucinations_rubric, hallucinations_binary],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hallucinations_metric': 0.5932, 'hallucinations_rubric': 3.1500, 'hallucinations_binary': 0.1000}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>hallucinations_metric</th>\n",
       "      <th>hallucinations_rubric</th>\n",
       "      <th>hallucinations_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the global implications of the USA Su...</td>\n",
       "      <td>[- In 2022, the USA Supreme Court handed down ...</td>\n",
       "      <td>The global implications of the USA Supreme Cou...</td>\n",
       "      <td>The global implications of the USA Supreme Cou...</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which companies are the main contributors to G...</td>\n",
       "      <td>[In recent years, there has been increasing pr...</td>\n",
       "      <td>According to the Carbon Majors database, the m...</td>\n",
       "      <td>According to the Carbon Majors database, the m...</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which private companies in the Americas are th...</td>\n",
       "      <td>[The issue of greenhouse gas emissions has bec...</td>\n",
       "      <td>According to the Carbon Majors database, the l...</td>\n",
       "      <td>The largest private companies in the Americas ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What action did Amnesty International urge its...</td>\n",
       "      <td>[In the case of the Ogoni 9, Amnesty Internati...</td>\n",
       "      <td>Amnesty International urged its supporters to ...</td>\n",
       "      <td>Amnesty International urged its supporters to ...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the recommendations made by Amnesty I...</td>\n",
       "      <td>[In recent years, Amnesty International has fo...</td>\n",
       "      <td>Amnesty International made several recommendat...</td>\n",
       "      <td>The recommendations made by Amnesty Internatio...</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What are the global implications of the USA Su...   \n",
       "1  Which companies are the main contributors to G...   \n",
       "2  Which private companies in the Americas are th...   \n",
       "3  What action did Amnesty International urge its...   \n",
       "4  What are the recommendations made by Amnesty I...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [- In 2022, the USA Supreme Court handed down ...   \n",
       "1  [In recent years, there has been increasing pr...   \n",
       "2  [The issue of greenhouse gas emissions has bec...   \n",
       "3  [In the case of the Ogoni 9, Amnesty Internati...   \n",
       "4  [In recent years, Amnesty International has fo...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The global implications of the USA Supreme Cou...   \n",
       "1  According to the Carbon Majors database, the m...   \n",
       "2  According to the Carbon Majors database, the l...   \n",
       "3  Amnesty International urged its supporters to ...   \n",
       "4  Amnesty International made several recommendat...   \n",
       "\n",
       "                                           reference  hallucinations_metric  \\\n",
       "0  The global implications of the USA Supreme Cou...               0.423077   \n",
       "1  According to the Carbon Majors database, the m...               0.862069   \n",
       "2  The largest private companies in the Americas ...               1.000000   \n",
       "3  Amnesty International urged its supporters to ...               0.400000   \n",
       "4  The recommendations made by Amnesty Internatio...               0.952381   \n",
       "\n",
       "   hallucinations_rubric  hallucinations_binary  \n",
       "0                      3                      0  \n",
       "1                      3                      0  \n",
       "2                      3                      0  \n",
       "3                      3                      0  \n",
       "4                      3                      0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = results.to_pandas()\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about how to build custom metrics, you can read the [Custom Metrics Advanced](./_write_your_own_metric_advanced.md) guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
